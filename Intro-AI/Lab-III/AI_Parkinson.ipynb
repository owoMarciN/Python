{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIFzT/A+zLISuTMhkiTUwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owoMarciN/Python/blob/main/Intro-AI/Lab-III/AI_Parkinson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pLDikd3j9zTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a518ee7d-c6e4-4914-9950-2d0ec2b8dea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: lion-pytorch in /usr/local/lib/python3.12/dist-packages (0.2.3)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.12/dist-packages (from lion-pytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion-pytorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6->lion-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6->lion-pytorch) (3.0.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.12/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (2.9.0+cu126)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torch_optimizer) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.3)\n",
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.11.12)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Requirement already satisfied: scikit-fuzzy in /usr/local/lib/python3.12/dist-packages (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install lion-pytorch\n",
        "!pip install torchvision\n",
        "!pip install torch_optimizer\n",
        "!pip install --upgrade ucimlrepo\n",
        "!pip install optuna\n",
        "!pip install xgboost\n",
        "!pip install -U scikit-fuzzy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo, list_available_datasets\n",
        "\n",
        "# check which datasets can be imported\n",
        "#list_available_datasets()\n",
        "\n",
        "# import dataset\n",
        "park = fetch_ucirepo(id=174)\n",
        "\n",
        "# access data\n",
        "X = park.data.features\n",
        "y = park.data.targets\n",
        "# train model e.g. sklearn.linear_model.LinearRegression().fit(X, y)\n",
        "\n",
        "# access metadata\n",
        "# print(park.metadata.uci_id)\n",
        "# print(park.metadata.num_instances)\n",
        "# print(park.metadata.additional_info.summary)\n",
        "\n",
        "# access variable info in tabular format\n",
        "# print(park.variables)"
      ],
      "metadata": {
        "id": "Il9HvPdP95rC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch_optimizer as toptim"
      ],
      "metadata": {
        "id": "OJfRJmxS95z5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SciKit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Dimension reduction\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "vL-H6LFy952D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "XlLHmtfK954n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the data\n",
        "\n",
        "#print(X.describe())\n",
        "feature_names = X.columns.values"
      ],
      "metadata": {
        "id": "K0dkMHI8-UD7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "##[PLOT] Creating a **Countplot** of the dataset features.\n",
        "#-------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "smpcKIGQmCBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution plot\n",
        "\n",
        "sns.countplot(x=y['status'])\n",
        "plt.title(\"Class distribution\")\n",
        "plt.xlabel(\"Classes (0 - Healthy, 1 - Parkinson)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nH6IoHH79568"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "##[PLOT] Creating a **Pairplot** of the dataset.\n",
        "#-------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "fchwfz-Cl6Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp_data = X[['MDVP:Fo', 'MDVP:Fhi', 'MDVP:Flo', 'MDVP:RAP', 'MDVP:PPQ']]\n",
        "\n",
        "sns.pairplot(pp_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N7qetY6X959G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "##[PLOT] Creating a **Correlation HeatMap** of the dataset.\n",
        "#-------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "f7KAKTIulyHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(y, pd.DataFrame):\n",
        "    df = pd.concat([X, y], axis=1)\n",
        "else:\n",
        "    df = X.copy()\n",
        "    df[\"target\"] = y\n",
        "\n",
        "# --- 2. Compute correlation matrix ---\n",
        "corr = df.corr(method=\"pearson\")  # or \"spearman\"\n",
        "\n",
        "# --- 3. Plot heatmap ---\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    annot=True,\n",
        "    cmap=\"coolwarm\",\n",
        "    vmin=-1, vmax=1,\n",
        "    linewidths=0.3,\n",
        "    square=True\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap – UCI Parkinson Dataset (ID=174)\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mo1l83PrihGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim=30, hidden=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def get_optimizer(name, params, lr, momentum=0.9):\n",
        "    name = name.lower()\n",
        "    if name == \"sgd\":\n",
        "        return optim.SGD(params, lr=lr)\n",
        "    if name == \"momentum\":\n",
        "        return optim.SGD(params, lr=lr, momentum=momentum)\n",
        "    if name == \"adam\":\n",
        "        return optim.Adam(params, lr=lr)\n",
        "    if name == \"rmsprop\":\n",
        "        return optim.RMSprop(params, lr=lr, momentum=momentum)\n",
        "    raise ValueError(\"Unknown optimizer\")"
      ],
      "metadata": {
        "id": "lxizaOpQAimg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(opt_name, lr, epochs, momentum, Xtr, Xte, ytr, yte, in_dim=30):\n",
        "    start_time = time.time()\n",
        "    model = MLP(in_dim=in_dim)\n",
        "    optimizer = get_optimizer(opt_name, model.parameters(), lr, momentum)\n",
        "    criterion = nn.BCELoss()\n",
        "    losses = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(Xtr)\n",
        "        loss = criterion(outputs, ytr)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_prob = model(Xte).detach().cpu().numpy()\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return losses, {\n",
        "            \"acc\": accuracy_score(yte, y_pred),\n",
        "            \"prec\": precision_score(yte, y_pred),\n",
        "            \"rec\": recall_score(yte, y_pred),\n",
        "            \"f1\": f1_score(yte, y_pred),\n",
        "            \"auc\": roc_auc_score(yte, y_prob),\n",
        "            \"mcc\": matthews_corrcoef(yte, y_pred)\n",
        "    }, elapsed_time"
      ],
      "metadata": {
        "id": "rp4n2DCeCGgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = StandardScaler().fit_transform(X).astype('float32')\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "Xtr, Xte = map(torch.tensor, (Xtr, Xte))\n",
        "ytr, yte = map (lambda v : torch.tensor(v.values).float(), (ytr, yte))"
      ],
      "metadata": {
        "id": "RdxZvOgdBSzK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "f1 = []\n",
        "auc = []\n",
        "mcc = []\n",
        "for _ in range(20):\n",
        "    losses, metrics, elapsed_time = train_and_eval(opt_name=\"adam\", lr=0.01, epochs=100, momentum=0.9, Xtr=Xtr, Xte=Xte, ytr=ytr, yte=yte, in_dim=Xtr.shape[1])\n",
        "    acc.append(metrics[\"acc\"])\n",
        "    f1.append(metrics[\"f1\"])\n",
        "    auc.append(metrics[\"auc\"])\n",
        "    mcc.append(metrics[\"mcc\"])\n",
        "\n",
        "print(np.mean(acc), np.std(acc))\n",
        "print(np.mean(f1), np.std(f1))\n",
        "print(np.mean(acc), np.std(auc))\n",
        "print(np.mean(mcc))"
      ],
      "metadata": {
        "id": "7INtpHAKCNc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(clf, Xtr, Xte, ytr, yte):\n",
        "    ytr = ytr.ravel()\n",
        "    yte = yte.ravel()\n",
        "\n",
        "    clf.fit(Xtr, ytr)\n",
        "\n",
        "    y_pred = clf.predict(Xte)\n",
        "\n",
        "    # For AUC\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        y_prob = clf.predict_proba(Xte)[:, 1]\n",
        "    elif hasattr(clf, \"decision_function\"):\n",
        "        y_prob = clf.decision_function(Xte)\n",
        "    else:\n",
        "        raise ValueError(\"Model does not support probability or decision_function.\")\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": accuracy_score(yte, y_pred),\n",
        "        \"prec\": precision_score(yte, y_pred),\n",
        "        \"rec\": recall_score(yte, y_pred),\n",
        "        \"f1\": f1_score(yte, y_pred),\n",
        "        \"auc\": roc_auc_score(yte, y_prob),\n",
        "        \"mcc\": matthews_corrcoef(yte, y_pred)\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "9J3nL5JnZLqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    random_state=None\n",
        ")\n",
        "\n",
        "svm = SVC(\n",
        "    kernel=\"rbf\",\n",
        "    probability=True,\n",
        "    C=1.0\n",
        ")\n",
        "\n",
        "xgb_clf = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    eval_metric=\"logloss\"\n",
        ")"
      ],
      "metadata": {
        "id": "IRC_1Xb7a6Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "f1 = []\n",
        "auc = []\n",
        "mcc = []\n",
        "\n",
        "for _ in range(20):\n",
        "    metrics = evaluate_model(xgb_clf, Xtr, Xte, ytr, yte)\n",
        "\n",
        "    acc.append(metrics[\"acc\"])\n",
        "    f1.append(metrics[\"f1\"])\n",
        "    auc.append(metrics[\"auc\"])\n",
        "    mcc.append(metrics[\"mcc\"])\n",
        "\n",
        "print(\"ACC:\", np.mean(acc), np.std(acc))\n",
        "print(\"F1:\", np.mean(f1), np.std(f1))\n",
        "print(\"AUC:\", np.mean(auc), np.std(auc))\n",
        "print(\"MCC:\", np.mean(mcc))"
      ],
      "metadata": {
        "id": "aLSivD48ZnNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "##[PLOT] Creating a **Boxplot** of the normalized data of the dataset.\n",
        "#-------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "HlH2WyH7lEcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "sns.boxplot(data=X)\n",
        "\n",
        "plt.xticks(\n",
        "    ticks=range(len(feature_names)),\n",
        "    labels=feature_names,\n",
        "    rotation=90,\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Normalized Value\")\n",
        "\n",
        "plt.title(\"Boxplots for All Features – UCI Parkinson Dataset\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FzgirGW6j8nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline MLP\n",
        "opt = [\"adam\", \"sgd\", \"momentum\", \"rmsprop\"]\n",
        "losses_all = []\n",
        "f1_all = []\n",
        "acc_all = []\n",
        "epochs = []\n",
        "elapsed_times = []\n",
        "\n",
        "for opt_name in opt:\n",
        "    losses, metrics, elapsed_time = train_and_eval(opt_name=opt_name, lr=0.01, epochs=100, momentum=0.9, Xtr=Xtr, Xte=Xte, ytr=ytr, yte=yte, in_dim=Xtr.shape[1])\n",
        "    f1_all.append(metrics[\"f1\"])\n",
        "    acc_all.append(metrics[\"acc\"])\n",
        "    losses_all.append(losses)\n",
        "    elapsed_times.append(elapsed_time)\n",
        "\n",
        "for i, opt_name in enumerate(opt):\n",
        "    plt.plot(losses_all[i], label=opt_name)\n",
        "plt.legend()\n",
        "plt.savefig(\"lossfun.png\")\n",
        "plt.show()\n",
        "\n",
        "for i, opt_name in enumerate(opt):\n",
        "    print(f\"{opt_name:10s}  acc={acc_all[i]:.4f}  f1={f1_all[i]:.4f}  time={elapsed_times[i]:.4f}s\")"
      ],
      "metadata": {
        "id": "qXMjka55DO7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(opt_name, lr, epochs, momentum, Xtr, Xte, ytr, yte, in_dim=30):\n",
        "    start_time = time.time()\n",
        "    model = MLP(in_dim=in_dim)\n",
        "    optimizer = get_optimizer(opt_name, model.parameters(), lr, momentum)\n",
        "    criterion = nn.BCELoss()\n",
        "    losses = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(Xtr)\n",
        "        loss = criterion(outputs, ytr)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_prob = model(Xte).numpy()\n",
        "        y_pred = (y_prob >= 0.5).astype(int)\n",
        "        f1_scores.append(f1_score(yte, y_pred))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_prob = model(Xte).numpy()\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return losses, {\n",
        "            \"acc\": accuracy_score(yte, y_pred),\n",
        "            \"prec\": precision_score(yte, y_pred),\n",
        "            \"rec\": recall_score(yte, y_pred),\n",
        "            \"f1\": f1_score(yte, y_pred),\n",
        "            \"auc\": roc_auc_score(yte, y_prob)\n",
        "    }, elapsed_time, f1_scores"
      ],
      "metadata": {
        "id": "a_dYUYU-98hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline MLP\n",
        "opt = [\"adam\", \"sgd\", \"momentum\", \"rmsprop\"]\n",
        "losses_all = []\n",
        "f1_all = []\n",
        "acc_all = []\n",
        "epochs = []\n",
        "elapsed_times = []\n",
        "\n",
        "for opt_name in opt:\n",
        "    losses, metrics, elapsed_time, f1_scores = train_and_eval(opt_name=opt_name, lr=0.01, epochs=100, momentum=0.9, Xtr=Xtr, Xte=Xte, ytr=ytr, yte=yte, in_dim=Xtr.shape[1])\n",
        "    f1_all.append(f1_scores)\n",
        "\n",
        "for i, opt_name in enumerate(opt):\n",
        "    plt.plot(f1_all[i], label=opt_name)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JQrVzzZV97ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def train_and_eval(opt_name, lr, epochs, momentum, Xtr, Xte, ytr, yte, in_dim=30):\n",
        "    start_time = time.time()\n",
        "    model = MLP(in_dim=in_dim)\n",
        "    optimizer = get_optimizer(opt_name, model.parameters(), lr, momentum)\n",
        "    criterion = nn.BCELoss()\n",
        "    losses = []\n",
        "    l_rates = []\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(Xtr)\n",
        "        loss = criterion(outputs, ytr)\n",
        "        loss.backward()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        l_rates.append(scheduler.get_last_lr())\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_prob = model(Xte).numpy()\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return losses, {\n",
        "            \"acc\": accuracy_score(yte, y_pred),\n",
        "            \"prec\": precision_score(yte, y_pred),\n",
        "            \"rec\": recall_score(yte, y_pred),\n",
        "            \"f1\": f1_score(yte, y_pred),\n",
        "            \"auc\": roc_auc_score(yte, y_prob)\n",
        "    }, elapsed_time, l_rates"
      ],
      "metadata": {
        "id": "uMLzQgCP_XQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline MLP\n",
        "opt = [\"adam\", \"sgd\", \"momentum\", \"rmsprop\"]\n",
        "losses_all = []\n",
        "f1_all = []\n",
        "acc_all = []\n",
        "epochs = []\n",
        "elapsed_times = []\n",
        "lr_all = []\n",
        "\n",
        "for opt_name in opt:\n",
        "    losses, metrics, elapsed_time, l_rates = train_and_eval(opt_name=opt_name, lr=0.01, epochs=100, momentum=0.9, Xtr=Xtr, Xte=Xte, ytr=ytr, yte=yte, in_dim=Xtr.shape[1])\n",
        "    f1_all.append(metrics[\"f1\"])\n",
        "    acc_all.append(metrics[\"acc\"])\n",
        "    losses_all.append(losses)\n",
        "    lr_all.append(l_rates)\n",
        "    elapsed_times.append(elapsed_time)\n",
        "\n",
        "for i, opt_name in enumerate(opt):\n",
        "    plt.plot(losses_all[i], label=opt_name)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for i, opt_name in enumerate(opt):\n",
        "    print(f\"{opt_name:10s}  acc={acc_all[i]:.4f}  f1={f1_all[i]:.4f}  time={elapsed_times[i]:.4f}s\")\n",
        "\n",
        "\n",
        "for i, opt_name in enumerate(opt):\n",
        "    plt.plot(lr_all[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OXuM8XQFATFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_features(X_train, X_test, n_clusters=8, random_state=42):\n",
        "    # Ensure inputs are NumPy arrays for KMeans and subsequent NumPy operations\n",
        "    if isinstance(X_train, torch.Tensor):\n",
        "        X_train_np = X_train.numpy()\n",
        "    else:\n",
        "        X_train_np = X_train\n",
        "\n",
        "    if isinstance(X_test, torch.Tensor):\n",
        "        X_test_np = X_test.numpy()\n",
        "    else:\n",
        "        X_test_np = X_test\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters,\n",
        "    n_init=10,\n",
        "    random_state=random_state)\n",
        "    kmeans.fit(X_train_np)\n",
        "\n",
        "    def to_dist(X_arr):\n",
        "        # X_arr will already be a NumPy array if the outer conversion happened\n",
        "        return np.linalg.norm(\n",
        "            X_arr[:, None, :] - kmeans.cluster_centers_[None, :, :],\n",
        "            axis=2\n",
        "            ).astype(\"float32\")\n",
        "\n",
        "    return to_dist(X_train_np), to_dist(X_test_np)"
      ],
      "metadata": {
        "id": "yioq_EtxJewU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr_km, Xte_km = kmeans_features(Xtr, Xte, n_clusters=8)\n",
        "\n",
        "Xtr_km = torch.tensor(Xtr_km, dtype=torch.float32)\n",
        "Xte_km = torch.tensor(Xte_km, dtype=torch.float32)\n",
        "\n",
        "losses, metrics, elapsed_time, l_rates = train_and_eval(opt_name=\"adam\", lr=0.01, epochs=100, momentum=0.9, Xtr=Xtr_km, Xte=Xte_km, ytr=ytr, yte=yte, in_dim=8)\n",
        "metrics"
      ],
      "metadata": {
        "id": "elH2xBObJkSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_features(X_train, X_test, var_threshold=0.95, random_state=42):\n",
        "    # PCA\n",
        "    pca = PCA(n_components=var_threshold, random_state=random_state)\n",
        "\n",
        "    pca.fit(X_train)\n",
        "\n",
        "    # Transformation train/test\n",
        "    X_train_pca = pca.transform(X_train).astype(\"float32\")\n",
        "    X_test_pca = pca.transform(X_test).astype(\"float32\")\n",
        "\n",
        "    # Number of components\n",
        "    n_components = pca.n_components_\n",
        "\n",
        "    print(f\"We used {n_components} PCA components, to get ≥{int(var_threshold*100)}% variation.\")\n",
        "\n",
        "    return X_train_pca, X_test_pca, n_components"
      ],
      "metadata": {
        "id": "v-MX0L_9JkVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr_pca, Xte_pca, n_components = pca_features(Xtr, Xte, var_threshold=0.95)\n",
        "\n",
        "Xtr_pca = torch.tensor(Xtr_pca, dtype=torch.float32)\n",
        "Xte_pca = torch.tensor(Xte_pca, dtype=torch.float32)\n",
        "\n",
        "losses, metrics, elapsed_time, l_rates = train_and_eval(opt_name=\"adam\", lr=0.01, epochs=100, momentum=0.9, Xtr=Xtr_pca, Xte=Xte_pca, ytr=ytr, yte=yte, in_dim=n_components)\n",
        "metrics"
      ],
      "metadata": {
        "id": "1zlSWsl2JkXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=30, bottleneck_dim=8):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, bottleneck_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Returns the 8-D bottleneck representation.\"\"\"\n",
        "        return self.encoder(x)"
      ],
      "metadata": {
        "id": "ie-LQfblJkZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder_features(X_train, X_test, epochs=50, lr=1e-3):\n",
        "    input_dim = X_train.shape[1]\n",
        "    bottleneck_dim = 8\n",
        "\n",
        "    ae = Autoencoder(input_dim=input_dim, bottleneck_dim=bottleneck_dim)\n",
        "    optimizer = optim.Adam(ae.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    Xtr_tensor = X_train.float()\n",
        "\n",
        "    # Training loop\n",
        "    for _ in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        recon = ae(Xtr_tensor)\n",
        "        loss = loss_fn(recon, Xtr_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Extract 8-D features\n",
        "    with torch.no_grad():\n",
        "        Z_train = ae.encode(X_train.float())\n",
        "        Z_test  = ae.encode(X_test.float())\n",
        "\n",
        "    return Z_train, Z_test"
      ],
      "metadata": {
        "id": "GE4K8NrUJkcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr_ae, Xte_ae = autoencoder_features(Xtr, Xte, epochs=100, lr=1e-3)\n",
        "\n",
        "losses, metrics, elapsed_time, l_rates = train_and_eval(opt_name=\"adam\", lr=0.01, epochs=100, momentum=0.9, Xtr=Xtr_ae, Xte=Xte_ae, ytr=ytr, yte=yte, in_dim=8)\n",
        "metrics"
      ],
      "metadata": {
        "id": "jCkCGp_6Jkef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "ytr_np = ytr.numpy().ravel()\n",
        "\n",
        "# PCA 2D\n",
        "pca_2d = PCA(n_components=2)\n",
        "Xtr_pca_2d = pca_2d.fit_transform(Xtr_pca)  # Xtr should be NumPy\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "for label in np.unique(ytr_np):\n",
        "    plt.scatter(Xtr_pca_2d[ytr_np == label, 0], Xtr_pca_2d[ytr_np == label, 1], label=f\"Class {label}\")\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"2D PCA Projection\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w5Zn3mvQ8iSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytr_np = ytr.numpy().ravel()\n",
        "\n",
        "# PCA 2D\n",
        "pca_2d = PCA(n_components=2)\n",
        "Xtr_ae_2d = pca_2d.fit_transform(Xtr_ae)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "for label in np.unique(ytr_np):\n",
        "    plt.scatter(Xtr_ae_2d[ytr_np == label, 0], Xtr_ae_2d[ytr_np == label, 1], label=f\"Class {label}\")\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"2D AE Projection\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7n6eh0s388ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameters\n"
      ],
      "metadata": {
        "id": "l9w40F23IXK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import optuna\n",
        "\n",
        "from lion_pytorch import Lion\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from optuna.samplers import TPESampler, NSGAIISampler, GridSampler, RandomSampler\n",
        "from optuna.pruners import HyperbandPruner"
      ],
      "metadata": {
        "id": "Riqu05wYE8rX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_space = {\n",
        "    \"lr\": [1e-4, 1e-3, 1e-2, 1e-1],\n",
        "    \"momentum\": [0.7, 0.8, 0.9, 0.99],\n",
        "    \"hidden_size\": [8, 16, 32],\n",
        "    \"weight_decay\": [0.001, 0.01],\n",
        "    \"batch_size\": [16, 32, 64]\n",
        "}"
      ],
      "metadata": {
        "id": "S4sdZGP7E7ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim=30, hidden=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def get_optimizer(name, params, lr, momentum=0.9, weight_decay=0.0):\n",
        "    \"\"\"Return PyTorch optimizer instance based on name.\"\"\"\n",
        "    name = name.lower()\n",
        "    # SGD variants\n",
        "    if name == \"sgd\":\n",
        "        return optim.SGD(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"momentum\":\n",
        "        return optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # Adaptive methods\n",
        "    if name == \"adam\":\n",
        "        return optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"amsgrad\":\n",
        "        return optim.Adam(params, lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
        "    if name == \"adagrad\":\n",
        "        return optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"adadelta\":\n",
        "        return optim.Adadelta(params, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Other advanced optimizers (requires torch_optimizer)\n",
        "    if name == \"nadam\":\n",
        "        return optim.NAdam(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"lion\":\n",
        "        return Lion(params, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # RMSprop\n",
        "    if name == \"rmsprop\":\n",
        "        return optim.RMSprop(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    raise ValueError(f\"Unknown optimizer: {name}\")\n",
        "\n",
        "def train_and_eval_hparams(opt_name, Xtr, Xte, ytr, yte, hparams):\n",
        "    start_time = time.time()\n",
        "    model = MLP(in_dim=Xtr.shape[1], hidden=hparams[\"hidden_size\"])\n",
        "    optimizer = get_optimizer(opt_name, model.parameters(), lr=hparams[\"lr\"], momentum=hparams[\"momentum\"], weight_decay=hparams[\"weight_decay\"])\n",
        "    criterion = nn.BCELoss()\n",
        "    losses = []\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(Xtr, ytr),\n",
        "        batch_size=hparams[\"batch_size\"],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    early_stop_patience = 10\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        batch_losses = []\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(batch_losses)\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "        if early_stop_patience is not None:\n",
        "            if epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            if patience_counter >= early_stop_patience:\n",
        "                # Early stoping if there is no change\n",
        "                break\n",
        "        epochs_run = epoch + 1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_prob = model(Xte).numpy()\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": accuracy_score(yte, y_pred),\n",
        "        \"prec\": precision_score(yte, y_pred, zero_division=0),\n",
        "        \"rec\": recall_score(yte, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(yte, y_pred, zero_division=0),\n",
        "        \"auc\": roc_auc_score(yte, y_prob),\n",
        "        \"epochs\": epochs_run\n",
        "    }\n",
        "\n",
        "    return metrics, losses, elapsed_time"
      ],
      "metadata": {
        "id": "xVXDSxRY3wKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(opt_name, Xtr, Xte, ytr, yte, trial):\n",
        "    hp = {\n",
        "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True),\n",
        "        \"momentum\": trial.suggest_float(\"momentum\", 0.7, 0.99),\n",
        "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [8, 16, 32]),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True),\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    }\n",
        "\n",
        "    metrics, losses, elapsed_time_val = train_and_eval_hparams(opt_name, Xtr, Xte, ytr, yte, hp)\n",
        "    trial.set_user_attr(\"losses\", losses)\n",
        "    trial.set_user_attr(\"elapsed_time\", elapsed_time_val)\n",
        "    trial.set_user_attr(\"optimizer\", opt_name)\n",
        "    trial.set_user_attr(\"metrics\", metrics)\n",
        "    return metrics[\"f1\"]"
      ],
      "metadata": {
        "id": "2ri9tTK9TB22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna Grid Search\n",
        "def run_optuna_grid_search(opt_name, Xtr, Xte, ytr, yte, n_trials=25):\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        sampler=GridSampler(search_space=search_space)\n",
        "    )\n",
        "\n",
        "    study.optimize(lambda trial: objective(opt_name, Xtr, Xte, ytr, yte, trial), n_trials=n_trials)\n",
        "\n",
        "    trajectory  = [t.value for t in study.trials]\n",
        "\n",
        "    # Get only the losses and metrics of the best trial\n",
        "    best_trial = study.best_trial\n",
        "    best_perf = best_trial.user_attrs[\"elapsed_time\"]\n",
        "    best_losses = best_trial.user_attrs[\"losses\"]\n",
        "    best_metrics = best_trial.user_attrs[\"metrics\"]\n",
        "\n",
        "    return study.best_params, study.best_value, trajectory, best_perf, best_losses, best_metrics"
      ],
      "metadata": {
        "id": "pHR7XkG4Cx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna Random Search\n",
        "def run_optuna_random_search(opt_name, Xtr, Xte, ytr, yte, n_trials=25):\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        sampler=RandomSampler()\n",
        "    )\n",
        "\n",
        "    study.optimize(lambda trial: objective(opt_name, Xtr, Xte, ytr, yte, trial), n_trials=n_trials)\n",
        "\n",
        "    trajectory  = [t.value for t in study.trials]\n",
        "\n",
        "    # Get only the losses and metrics of the best trial\n",
        "    best_trial = study.best_trial\n",
        "    best_perf = best_trial.user_attrs[\"elapsed_time\"]\n",
        "    best_losses = best_trial.user_attrs[\"losses\"]\n",
        "    best_metrics = best_trial.user_attrs[\"metrics\"]\n",
        "\n",
        "    return study.best_params, study.best_value, trajectory, best_perf, best_losses, best_metrics"
      ],
      "metadata": {
        "id": "f9BG32RHCyCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayesian Optimization — Build surrogate model of performance.\n",
        "def run_bayesian_optimization(opt_name, Xtr, Xte, ytr, yte, n_trials=25):\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        sampler=TPESampler()\n",
        "    )\n",
        "    study.optimize(lambda trial: objective(opt_name, Xtr, Xte, ytr, yte, trial), n_trials=n_trials)\n",
        "\n",
        "    trajectory  = [t.value for t in study.trials]\n",
        "\n",
        "    # Get only the losses and metrics of the best trial\n",
        "    best_trial = study.best_trial\n",
        "    best_perf = best_trial.user_attrs[\"elapsed_time\"]\n",
        "    best_losses = best_trial.user_attrs[\"losses\"]\n",
        "    best_metrics = best_trial.user_attrs[\"metrics\"]\n",
        "\n",
        "    return study.best_params, study.best_value, trajectory, best_perf, best_losses, best_metrics"
      ],
      "metadata": {
        "id": "_h79lGMICyGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperband / BOHB — Adaptive resource allocation.\n",
        "def run_hyperband(opt_name, Xtr, Xte, ytr, yte, n_trials=25):\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        pruner=HyperbandPruner()\n",
        "    )\n",
        "\n",
        "    study.optimize(lambda trial: objective(opt_name, Xtr, Xte, ytr, yte, trial), n_trials=n_trials)\n",
        "\n",
        "    trajectory  = [t.value for t in study.trials]\n",
        "\n",
        "    # Get only the losses and metrics of the best trial\n",
        "    best_trial = study.best_trial\n",
        "    best_perf = best_trial.user_attrs[\"elapsed_time\"]\n",
        "    best_losses = best_trial.user_attrs[\"losses\"]\n",
        "    best_metrics = best_trial.user_attrs[\"metrics\"]\n",
        "\n",
        "    return study.best_params, study.best_value, trajectory, best_perf, best_losses, best_metrics"
      ],
      "metadata": {
        "id": "tRqa5JKkCyJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evolutionary / Metaheuristic Search — GA, PSO, DE for AutoML.\n",
        "def run_evolutionary(opt_name, Xtr, Xte, ytr, yte, n_trials=25):\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        sampler=NSGAIISampler()\n",
        "    )\n",
        "\n",
        "    study.optimize(lambda trial: objective(opt_name, Xtr, Xte, ytr, yte, trial), n_trials=n_trials)\n",
        "\n",
        "    trajectory  = [t.value for t in study.trials]\n",
        "\n",
        "    # Get only the losses and metrics of the best trial\n",
        "    best_trial = study.best_trial\n",
        "    best_perf = best_trial.user_attrs[\"elapsed_time\"]\n",
        "    best_losses = best_trial.user_attrs[\"losses\"]\n",
        "    best_metrics = best_trial.user_attrs[\"metrics\"]\n",
        "\n",
        "    return study.best_params, study.best_value, trajectory, best_perf, best_losses, best_metrics"
      ],
      "metadata": {
        "id": "cFpOJMLnCyND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "# As for now testing the baseline with 'Adam' optimizer\n",
        "adam_opt = \"adam\"\n",
        "\n",
        "# Baselines\n",
        "grid_hp, grid_f1, grid_traj, grid_perf, grid_loss, grid_met = run_optuna_grid_search(\n",
        "            opt_name=adam_opt,\n",
        "            Xtr=Xtr,\n",
        "            Xte=Xte,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "results[\"Grid\"] = grid_hp\n",
        "results[\"Grid_f1\"] = grid_f1\n",
        "results[\"Grid_traj\"] = grid_traj\n",
        "results[\"Grid_perf\"] = grid_perf\n",
        "results[\"Grid_loss\"] = grid_loss\n",
        "results[\"Grid_met\"] = grid_met\n",
        "\n",
        "rand_hp, rand_f1, rand_traj, rand_perf, rand_loss, rand_met = run_optuna_random_search(\n",
        "            opt_name=adam_opt,\n",
        "            Xtr=Xtr,\n",
        "            Xte=Xte,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "results[\"Random\"] = rand_hp\n",
        "results[\"Random_f1\"] = rand_f1\n",
        "results[\"Random_traj\"] = rand_traj\n",
        "results[\"Random_perf\"] = rand_perf\n",
        "results[\"Random_loss\"] = rand_loss\n",
        "results[\"Random_met\"] = rand_met\n",
        "\n",
        "# Advanced methods\n",
        "bo_hp, bo_f1, bo_traj, bo_perf, bo_loss, bo_met = run_bayesian_optimization(\n",
        "            opt_name=adam_opt,\n",
        "            Xtr=Xtr,\n",
        "            Xte=Xte,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "results[\"Bayes\"] = bo_hp\n",
        "results[\"Bayes_f1\"] = bo_f1\n",
        "results[\"Bayes_traj\"] = bo_traj\n",
        "results[\"Bayes_perf\"] = bo_perf\n",
        "results[\"Bayes_loss\"] = bo_loss\n",
        "results[\"Bayes_met\"] = bo_met\n",
        "\n",
        "hb_hp, hb_f1, hb_traj, hb_perf, hb_loss, hb_met = run_hyperband(\n",
        "            opt_name=adam_opt,\n",
        "            Xtr=Xtr,\n",
        "            Xte=Xte,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "results[\"Hyperband\"] = hb_hp\n",
        "results[\"Hyperband_f1\"] = hb_f1\n",
        "results[\"Hyperband_traj\"] = hb_traj\n",
        "results[\"Hyperband_perf\"] = hb_perf\n",
        "results[\"Hyperband_loss\"] = hb_loss\n",
        "results[\"Hyperband_met\"] = hb_met\n",
        "\n",
        "ev_hp, ev_f1, ev_traj, ev_perf, ev_loss, ev_met = run_evolutionary(\n",
        "            opt_name=adam_opt,\n",
        "            Xtr=Xtr,\n",
        "            Xte=Xte,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "results[\"Evolutionary\"] = ev_hp\n",
        "results[\"Evolutionary_f1\"] = ev_f1\n",
        "results[\"Evolutionary_traj\"] = ev_traj\n",
        "results[\"Evolutionary_perf\"] = ev_perf\n",
        "results[\"Evolutionary_loss\"] = ev_loss\n",
        "results[\"Evolutionary_met\"] = ev_met"
      ],
      "metadata": {
        "id": "plRMIdJbEvFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Truncate trajectories and performances for consistent plotting, as Optuna\n",
        "# Grid Search runs all combinations while other methods are limited by\n",
        "# n_trials (defaulting to 25).\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "N_TRIALS = 25\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results[\"Grid_traj\"][:N_TRIALS], label=\"Grid Search\")\n",
        "plt.plot(results[\"Random_traj\"][:N_TRIALS], label=\"Random Search\")\n",
        "plt.plot(results[\"Bayes_traj\"][:N_TRIALS], label=\"Bayesian Opt (TPE)\")\n",
        "plt.plot(results[\"Hyperband_traj\"][:N_TRIALS], label=\"Hyperband\")\n",
        "plt.plot(results[\"Evolutionary_traj\"][:N_TRIALS], label=\"Evolutionary (NSGAII)\")\n",
        "\n",
        "plt.ylim([0.7, 1.0])\n",
        "plt.xlabel(\"Trial\")\n",
        "plt.ylabel(\"Best F1\")\n",
        "plt.title(\"Performance Trajectory\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Changes to the evaluation class must be made if you want the 'elapsed_time'\n",
        "# trajectories.\n",
        "# For now 'grid_perf' returns only the best trial performance time.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(results[\"Grid_perf\"][:N_TRIALS], label=\"Grid Search\")\n",
        "# plt.plot(results[\"Random_perf\"][:N_TRIALS], label=\"Random Search\")\n",
        "# plt.plot(results[\"Bayes_perf\"][:N_TRIALS], label=\"Bayesian Opt (TPE)\")\n",
        "# plt.plot(results[\"Hyperband_perf\"][:N_TRIALS], label=\"Hyperband\")\n",
        "# plt.plot(results[\"Evolutionary_perf\"][:N_TRIALS], label=\"Evolutionary (NSGAII)\")\n",
        "\n",
        "# plt.xlabel(\"Trial\")\n",
        "# plt.ylabel(\"Elapsed Time (s)\")\n",
        "# plt.title(\"Elapsed Time per Trial\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "mYWXTH33Evsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = pd.DataFrame({\n",
        "    \"Method\": [\"Grid\", \"Random\", \"Bayes\", \"Hyperband\", \"Evolutionary\"],\n",
        "    \"Best F1\": [results[\"Grid_met\"][\"f1\"], results[\"Random_met\"][\"f1\"], results[\"Bayes_met\"][\"f1\"], results[\"Hyperband_met\"][\"f1\"], results[\"Evolutionary_met\"][\"f1\"]],\n",
        "    \"Accuracy\": [results[\"Grid_met\"][\"acc\"], results[\"Random_met\"][\"acc\"], results[\"Bayes_met\"][\"acc\"], results[\"Hyperband_met\"][\"acc\"], results[\"Evolutionary_met\"][\"acc\"]],\n",
        "    \"Precision\": [results[\"Grid_met\"][\"prec\"], results[\"Random_met\"][\"prec\"], results[\"Bayes_met\"][\"prec\"], results[\"Hyperband_met\"][\"prec\"], results[\"Evolutionary_met\"][\"prec\"]],\n",
        "    \"Recall\": [results[\"Grid_met\"][\"rec\"], results[\"Random_met\"][\"rec\"], results[\"Bayes_met\"][\"rec\"], results[\"Hyperband_met\"][\"rec\"], results[\"Evolutionary_met\"][\"rec\"]],\n",
        "    \"AUC\": [results[\"Grid_met\"][\"auc\"], results[\"Random_met\"][\"auc\"], results[\"Bayes_met\"][\"auc\"], results[\"Hyperband_met\"][\"auc\"], results[\"Evolutionary_met\"][\"auc\"]],\n",
        "    \"Epochs\":  [results[\"Grid_met\"][\"epochs\"], results[\"Random_met\"][\"epochs\"], results[\"Bayes_met\"][\"epochs\"], results[\"Hyperband_met\"][\"epochs\"], results[\"Evolutionary_met\"][\"epochs\"]],\n",
        "    \"Ran-Time\": [results[\"Grid_perf\"], results[\"Random_perf\"], results[\"Bayes_perf\"], results[\"Hyperband_perf\"], results[\"Evolutionary_perf\"]],\n",
        "    \"Best Params\": [\n",
        "        results[\"Grid\"], results[\"Random\"], results[\"Bayes\"], results[\"Hyperband\"], results[\"Evolutionary\"]\n",
        "    ]\n",
        "\n",
        "})\n",
        "\n",
        "print(table)\n",
        "table.to_latex(index=False)"
      ],
      "metadata": {
        "id": "amYcxbOAEziT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Everything together - Optimizers $\\times$ Reduction Techniques $\\times$ Hyper-parameters\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRNr8TopVdKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "opt = [\n",
        "    \"sgd\",\n",
        "    \"momentum\",\n",
        "    \"adam\",\n",
        "    \"amsgrad\",\n",
        "    \"adagrad\",\n",
        "    \"adadelta\",\n",
        "    \"nadam\",\n",
        "    \"lion\",\n",
        "    \"rmsprop\"\n",
        "]\n",
        "\n",
        "features = {\n",
        "    \"raw\": (Xtr, Xte),\n",
        "    \"kmeans\": kmeans_features(Xtr, Xte, n_clusters=8),\n",
        "    \"pca\": pca_features(Xtr, Xte, var_threshold=0.95)[:2],\n",
        "    \"autoencoder\": autoencoder_features(Xtr, Xte, epochs=100, lr=1e-2)\n",
        "}\n",
        "\n",
        "# Convert all to tensors\n",
        "for key, (Xtr_feat, Xte_feat) in features.items():\n",
        "    features[key] = (torch.tensor(Xtr_feat, dtype=torch.float32),\n",
        "                     torch.tensor(Xte_feat, dtype=torch.float32))\n",
        "\n",
        "for feat_name, (Xtr_feat, Xte_feat) in features.items():\n",
        "    results[feat_name] = {}\n",
        "\n",
        "    for opt_name in opt:\n",
        "        results[feat_name][opt_name] = {}  # Create sub-dict for each optimizer\n",
        "\n",
        "        # Baselines\n",
        "        grid_hp, grid_f1, grid_traj, grid_perf, grid_loss, grid_met = run_optuna_grid_search(\n",
        "            opt_name=opt_name,\n",
        "            Xtr=Xtr_feat,\n",
        "            Xte=Xte_feat,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "        results[feat_name][opt_name][\"Grid\"] = grid_hp\n",
        "        results[feat_name][opt_name][\"Grid_f1\"] = grid_f1\n",
        "        results[feat_name][opt_name][\"Grid_traj\"] = grid_traj\n",
        "        results[feat_name][opt_name][\"Grid_perf\"] = grid_perf\n",
        "        results[feat_name][opt_name][\"Grid_loss\"] = grid_loss\n",
        "        results[feat_name][opt_name][\"Grid_met\"] = grid_met\n",
        "\n",
        "        rand_hp, rand_f1, rand_traj, rand_perf, rand_loss, rand_met = run_optuna_random_search(\n",
        "            opt_name=opt_name,\n",
        "            Xtr=Xtr_feat,\n",
        "            Xte=Xte_feat,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "        results[feat_name][opt_name][\"Random\"] = rand_hp\n",
        "        results[feat_name][opt_name][\"Random_f1\"] = rand_f1\n",
        "        results[feat_name][opt_name][\"Random_traj\"] = rand_traj\n",
        "        results[feat_name][opt_name][\"Random_perf\"] = rand_perf\n",
        "        results[feat_name][opt_name][\"Random_loss\"] = rand_loss\n",
        "        results[feat_name][opt_name][\"Random_met\"] = rand_met\n",
        "\n",
        "        # Advanced methods\n",
        "        bo_hp, bo_f1, bo_traj, bo_perf, bo_loss, bo_met = run_bayesian_optimization(\n",
        "            opt_name=opt_name,\n",
        "            Xtr=Xtr_feat,\n",
        "            Xte=Xte_feat,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "        results[feat_name][opt_name][\"Bayes\"] = bo_hp\n",
        "        results[feat_name][opt_name][\"Bayes_f1\"] = bo_f1\n",
        "        results[feat_name][opt_name][\"Bayes_traj\"] = bo_traj\n",
        "        results[feat_name][opt_name][\"Bayes_perf\"] = bo_perf\n",
        "        results[feat_name][opt_name][\"Bayes_loss\"] = bo_loss\n",
        "        results[feat_name][opt_name][\"Bayes_met\"] = bo_met\n",
        "\n",
        "        hb_hp, hb_f1, hb_traj, hb_perf, hb_loss, hb_met = run_hyperband(\n",
        "            opt_name=opt_name,\n",
        "            Xtr=Xtr_feat,\n",
        "            Xte=Xte_feat,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "        results[feat_name][opt_name][\"Hyperband\"] = hb_hp\n",
        "        results[feat_name][opt_name][\"Hyperband_f1\"] = hb_f1\n",
        "        results[feat_name][opt_name][\"Hyperband_traj\"] = hb_traj\n",
        "        results[feat_name][opt_name][\"Hyperband_perf\"] = hb_perf\n",
        "        results[feat_name][opt_name][\"Hyperband_loss\"] = hb_loss\n",
        "        results[feat_name][opt_name][\"Hyperband_met\"] = hb_met\n",
        "\n",
        "        ev_hp, ev_f1, ev_traj, ev_perf, ev_loss, ev_met = run_evolutionary(\n",
        "            opt_name=opt_name,\n",
        "            Xtr=Xtr_feat,\n",
        "            Xte=Xte_feat,\n",
        "            ytr=ytr,\n",
        "            yte=yte,\n",
        "            n_trials=25\n",
        "        )\n",
        "        results[feat_name][opt_name][\"Evolutionary\"] = ev_hp\n",
        "        results[feat_name][opt_name][\"Evolutionary_f1\"] = ev_f1\n",
        "        results[feat_name][opt_name][\"Evolutionary_traj\"] = ev_traj\n",
        "        results[feat_name][opt_name][\"Evolutionary_perf\"] = ev_perf\n",
        "        results[feat_name][opt_name][\"Evolutionary_loss\"] = ev_loss\n",
        "        results[feat_name][opt_name][\"Evolutionary_met\"] = ev_met"
      ],
      "metadata": {
        "id": "fVEI40sFJvpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "features = [\"raw\", \"kmeans\", \"pca\", \"autoencoder\"]\n",
        "optimizers = [\n",
        "    \"sgd\",\n",
        "    \"momentum\",\n",
        "    \"adam\",\n",
        "    \"amsgrad\",\n",
        "    \"adagrad\",\n",
        "    \"adadelta\",\n",
        "    \"nadam\",\n",
        "    \"lion\",\n",
        "    \"rmsprop\"\n",
        "]\n",
        "\n",
        "hp_algos = [\"Grid\", \"Random\", \"Bayes\", \"Hyperband\", \"Evolutionary\"]\n",
        "algos = [\"Grid\", \"Random\", \"TPE\", \"Hyperband\", \"NSGAII\"]\n",
        "\n",
        "for feat in features:\n",
        "    plt.figure(figsize=(16,10))\n",
        "    for opt in optimizers:\n",
        "        for i in range(len(hp_algos)):\n",
        "            loss_curve = results[feat][opt][hp_algos[i] + \"_loss\"]\n",
        "            label_name = f\"{opt}+{algos[i]}\"\n",
        "            plt.plot(loss_curve, label=label_name)\n",
        "\n",
        "    plt.ylim([0, 1])\n",
        "    plt.title(f\"Loss Curves per Epoch (Best Trial) for {feat.upper()}\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(fontsize=6, ncol=3)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "s_p4d3zmQW5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compact_params(hp_dict):\n",
        "    # Convert long dict to short string, rounding floats\n",
        "    if hp_dict is None:\n",
        "        return \"\"\n",
        "    return \", \".join(f\"{k}={round(v, 4) if isinstance(v, float) else v}\" for k, v in hp_dict.items())\n",
        "\n",
        "rows = []\n",
        "\n",
        "for feat_name, feat_dict in results.items():\n",
        "    for opt_name, opt_dict in feat_dict.items():\n",
        "        for algo in [\"Grid\", \"Random\", \"Bayes\", \"Hyperband\", \"Evolutionary\"]:\n",
        "          time = opt_dict.get(f\"{algo}_perf\", None)\n",
        "          met = opt_dict.get(f\"{algo}_met\", None)\n",
        "          best_hp = opt_dict.get(algo, None)\n",
        "          if met is not None:\n",
        "              rows.append({\n",
        "                  \"Feature\": feat_name,\n",
        "                  \"Optimizer\": opt_name,\n",
        "                  \"HP Algorithm\": algo,\n",
        "                  \"Accuracy\": round(met.get(\"acc\", 0), 4),\n",
        "                  \"Precision\": round(met.get(\"prec\", 0), 4),\n",
        "                  \"Recall\": round(met.get(\"rec\", 0), 4),\n",
        "                  \"F1\": round(met.get(\"f1\", 0), 4),\n",
        "                  \"AUC\": round(met.get(\"auc\", 0), 4),\n",
        "                  \"Epochs\": met.get(\"epochs\", 0),\n",
        "                  \"Run-Time\": round(time, 4),\n",
        "                  \"Best Params\": compact_params(best_hp)\n",
        "                })\n",
        "\n",
        "table_clean = pd.DataFrame(rows)\n",
        "\n",
        "# Export to LaTeX\n",
        "latex_code = table_clean.to_latex(index=False, longtable=True)\n",
        "print(latex_code)"
      ],
      "metadata": {
        "id": "7DHJrDQgGtOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "park = fetch_ucirepo(id=174)\n",
        "\n",
        "# access data\n",
        "X = park.data.features\n",
        "y = park.data.targets\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values).float().ravel() # Extract values and flatten before converting to tensor\n",
        "\n",
        "# Split data\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GagfV17Kwy6X"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAW - ADAM - NSGAII 1.0000 1.0000 1.0000 1.0000 1.0000 91 0.4446 0.0126 – 32 0.0093 64\n",
        "import skfuzzy as fuzz\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim=22, hidden=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Optimizer Factory\n",
        "# ------------------------------\n",
        "# The original function is correct and requires no change.\n",
        "def get_optimizer(name, params, lr, momentum=0.9, weight_decay=0.0):\n",
        "    name = name.lower()\n",
        "    if name == \"sgd\":\n",
        "        return optim.SGD(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"momentum\":\n",
        "        return optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    if name == \"adam\":\n",
        "        return optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"amsgrad\":\n",
        "        return optim.Adam(params, lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
        "    if name == \"adagrad\":\n",
        "        return optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"adadelta\":\n",
        "        return optim.Adadelta(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"nadam\":\n",
        "        return optim.NAdam(params, lr=lr, weight_decay=weight_decay)\n",
        "    if name == \"rmsprop\":\n",
        "        return optim.RMSprop(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    raise ValueError(f\"Unknown optimizer: {name}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Fuzzy Reasoning Function\n",
        "# ------------------------------\n",
        "def fuzzy_score(X):\n",
        "    \"\"\"\n",
        "    Compute fuzzy membership scores for selected features.\n",
        "    The example features (0 and 5) correspond to 'MDVP:Fo(Hz)' and 'MDVP:Shimmer'\n",
        "    in the Parkinson's dataset (after dropping 'name' and 'status').\n",
        "    \"\"\"\n",
        "    X_np = X.numpy()\n",
        "    f1 = X_np[:, 0]  # MDVP:Fo(Hz) - mean fundamental frequency\n",
        "    f2 = X_np[:, 5]  # MDVP:Shimmer - local shimmer variability\n",
        "\n",
        "    # Note: These fuzzy membership functions and parameters are defined *after* # the data has been scaled by StandardScaler, which typically centers the\n",
        "    # data around 0. The original parameters (e.g., 90, 80, 140) are likely\n",
        "    # based on the *unscaled* data. Since the data is scaled, we adjust the\n",
        "    # logic to be more generic, assuming high values imply higher risk.\n",
        "    # For scaled data, values like -1, 0, 1 are more appropriate.\n",
        "\n",
        "    # Let's assume a higher value of f1 (mean freq) and f2 (shimmer) indicates a higher risk.\n",
        "    # We use a simple Sigmoidal membership (which is often done for scaled data)\n",
        "\n",
        "    # Simplified High-Risk Membership for scaled data (~1 to 2 standard deviations above mean)\n",
        "    mu_high_f1 = 1 / (1 + np.exp(-5 * (f1 - 1.0))) # Higher f1 -> higher membership\n",
        "    mu_high_f2 = 1 / (1 + np.exp(-5 * (f2 - 1.5))) # Higher f2 -> higher membership\n",
        "\n",
        "    # Weighted sum as fuzzy score (Higher R implies higher risk)\n",
        "    R = 0.5 * mu_high_f1 + 0.5 * mu_high_f2\n",
        "\n",
        "    # Ensure R is between 0 and 1\n",
        "    R = np.clip(R, 0, 1)\n",
        "    return torch.tensor(R, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Rule-Based Overrides\n",
        "# ------------------------------\n",
        "def rule_override(p, X):\n",
        "    \"\"\"\n",
        "    Override probabilities based on critical feature patterns.\n",
        "    e.g., If 'MDVP:Fo(Hz)' (X[:,0]) is very low (e.g., < -1 after scaling) AND\n",
        "    'HNR' (X[:,5] if it was the 6th feature, which is 'MDVP:Shimmer' in our setup)\n",
        "    is also very low, force a high risk probability (0.99).\n",
        "    \"\"\"\n",
        "    p_new = p.clone()\n",
        "    # In the original code, HNR was feature 5. In the UCI dataset with 22 features,\n",
        "    # HNR is actually the 17th feature (index 16). Feature 5 is 'MDVP:Shimmer'.\n",
        "    # We will use the intended features for a strong rule:\n",
        "    # Feature 0: MDVP:Fo(Hz) (Mean Freq)\n",
        "    # Feature 16: HNR (Harmonics to Noise Ratio)\n",
        "\n",
        "    # Rule: Very low mean frequency AND very low HNR -> high risk (Parkinson's)\n",
        "    # We use scaled values: < -1 is approx 1 standard deviation below mean.\n",
        "    critical_mask = (X[:, 0] < -1.0) & (X[:, 16] < -1.0)\n",
        "    p_new[critical_mask] = 0.99\n",
        "    return p_new\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Bayesian Posterior Update\n",
        "# ------------------------------\n",
        "def bayesian_update(p_likelihood, prior=0.2):\n",
        "    \"\"\"\n",
        "    Compute posterior probability using Bayes theorem (P(D|E)).\n",
        "    D = Disease (Parkinson's), E = Evidence (Hybrid MLP/Fuzzy Score)\n",
        "    p_likelihood is P(E|D) if we assume the score represents the likelihood of the evidence E\n",
        "    given the disease D, but in classification, it's usually P(D|E).\n",
        "\n",
        "    A more common interpretation in this context is treating the hybrid score p_rule\n",
        "    as the *likelihood* of the diagnosis (P(D|Data)) which is then refined.\n",
        "\n",
        "    The formula used in the code is for P(D|E) where E is the likelihood score itself:\n",
        "    P(D|E) = (P(E|D) * P(D)) / (P(E|D)*P(D) + P(E|~D)*P(~D))\n",
        "\n",
        "    Assuming p_likelihood *is* $P(D|E)$ and $P(\\sim D|E) = 1 - P(D|E)$:\n",
        "    The code's intent is to update the *prior probability* $P(D)$ with new *evidence* $E$.\n",
        "    However, the provided formula is:\n",
        "    $$P_{posterior} = \\frac{P_{likelihood} \\cdot Prior}{P_{likelihood} \\cdot Prior + (1 - P_{likelihood}) \\cdot (1 - Prior) + \\epsilon}$$\n",
        "\n",
        "    This is effectively treating $P_{likelihood}$ as $P(E|D)$ and $1-P_{likelihood}$ as $P(E|\\sim D)$,\n",
        "    which is a **simplification** often used in hybrid systems to adjust the score\n",
        "    based on a known base rate ($Prior$).\n",
        "    \"\"\"\n",
        "    P_E_given_D = p_likelihood # Simplified\n",
        "    P_E_given_notD = 1 - p_likelihood # Simplified\n",
        "\n",
        "    # Prior probability of having the condition (Parkinson's) - P(D)\n",
        "    P_D = prior\n",
        "    # Prior probability of not having the condition - P(~D)\n",
        "    P_notD = 1 - prior\n",
        "\n",
        "    # Bayes' Theorem for Posterior P(D|E)\n",
        "    numerator = P_E_given_D * P_D\n",
        "    denominator = numerator + P_E_given_notD * P_notD + 1e-8\n",
        "\n",
        "    posterior = numerator / denominator\n",
        "    return posterior\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Train & Evaluate Hybrid MLP\n",
        "# ------------------------------\n",
        "def train_and_eval_hybrid(opt_name, Xtr, Xte, ytr, yte, hparams, in_dim):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Ensure yte is integer for metrics where needed\n",
        "    yte_int = yte.int().numpy()\n",
        "\n",
        "    model = MLP(in_dim=in_dim, hidden=hparams[\"hidden_size\"])\n",
        "    optimizer = get_optimizer(opt_name, model.parameters(), lr=hparams[\"lr\"], momentum=hparams[\"momentum\"], weight_decay=hparams[\"weight_decay\"])\n",
        "    criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(Xtr, ytr),\n",
        "        batch_size=hparams[\"batch_size\"],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    print(f\"Starting training with {opt_name}...\")\n",
        "    for epoch in range(hparams[\"epochs\"]):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(xb).squeeze()\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # ---------- Hybrid Reasoning Layer (Inference) ----------\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        p = model(Xte).squeeze()                # Step 10: MLP output (probability $P_{MLP}$)\n",
        "        R = fuzzy_score(Xte)                    # Step 11: Fuzzy score ($P_{Fuzzy}$)\n",
        "\n",
        "        # Combine MLP + Fuzzy: Weighted Average\n",
        "        # This is the 'Evidence' (E) that will be fed into the Bayesian update\n",
        "        p_hybrid = hparams[\"mlp_weight\"] * p + hparams[\"fuzzy_weight\"] * R\n",
        "\n",
        "        p_rule = rule_override(p_hybrid, Xte)   # Step 12: Rule-based overrides ($P_{Rule}$)\n",
        "\n",
        "        # Use the rule-overridden score as the likelihood for the Bayesian update\n",
        "        posterior = bayesian_update(p_rule, prior=hparams[\"prior\"]) # Step 13: Bayesian posterior ($P_{Posterior}$)\n",
        "\n",
        "    # Final prediction based on a threshold of 0.5\n",
        "    y_pred = (posterior >= 0.5).int().numpy()\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": accuracy_score(yte_int, y_pred),\n",
        "        \"prec\": precision_score(yte_int, y_pred, zero_division=0),\n",
        "        \"rec\": recall_score(yte_int, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(yte_int, y_pred, zero_division=0),\n",
        "        \"auc\": roc_auc_score(yte_int, posterior.numpy()),\n",
        "        \"mcc\": matthews_corrcoef(yte_int, y_pred),\n",
        "        \"time\": duration\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnHPRs2ShyMW",
        "outputId": "265050f4-1531-463a-9934-d37305ccb131"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:113: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:113: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-360239415.py:113: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  Assuming p_likelihood *is* $P(D|E)$ and $P(\\sim D|E) = 1 - P(D|E)$:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAW - ADAM - NSGAII 1.0000 1.0000 1.0000 1.0000 1.0000 91 0.4446 0.0126 – 32 0.0093 6\n",
        "# Define hyperparameters for RAW features, ADAM optimizer\n",
        "hp = {\n",
        "    \"lr\": 0.0126,\n",
        "    \"momentum\": 0.9,\n",
        "    \"hidden_size\": 32,\n",
        "    \"weight_decay\": 0.0093,\n",
        "    \"batch_size\": 64,\n",
        "    \"mlp_weight\": 0.6, # Default weight for MLP output\n",
        "    \"fuzzy_weight\": 0.4, # Default weight for Fuzzy score\n",
        "    \"prior\": ytr.mean().item(), # Adjust prior to reflect actual positive class proportion in training data\n",
        "    \"epochs\": 100 # Number of training epochs\n",
        "}\n",
        "\n",
        "acc = []\n",
        "f1 = []\n",
        "auc = []\n",
        "mcc = []\n",
        "\n",
        "for _ in range(20):\n",
        "    metrics = train_and_eval_hybrid(\"adam\", Xtr, Xte, ytr, yte, hp, in_dim=Xtr.shape[1])\n",
        "    acc.append(metrics[\"acc\"])\n",
        "    f1.append(metrics[\"f1\"])\n",
        "    auc.append(metrics[\"auc\"])\n",
        "    mcc.append(metrics[\"mcc\"])\n",
        "\n",
        "print(\"ACC: {:.4f} ± {:.4f}\".format(np.mean(acc), np.std(acc)))\n",
        "print(\"F1: {:.4f} ± {:.4f}\".format(np.mean(f1), np.std(f1)))\n",
        "print(\"AUC: {:.4f} ± {:.4f}\".format(np.mean(auc), np.std(auc)))\n",
        "print(\"MCC: {:.4f} ± {:.4f}\".format(np.mean(mcc), np.std(mcc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKvvVHbrnXto",
        "outputId": "8e85e6ec-bb70-4b9d-a774-360e1cddecea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "Starting training with adam...\n",
            "Training complete.\n",
            "ACC: 0.9231 ± 0.0000\n",
            "F1: 0.9552 ± 0.0000\n",
            "AUC: 0.7190 ± 0.0106\n",
            "MCC: 0.7228 ± 0.0000\n"
          ]
        }
      ]
    }
  ]
}